import gymnasium as gym
from gymnasium import spaces
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
from stable_baselines3.common.vec_env import DummyVecEnv,VecNormalize
from stable_baselines3.common.callbacks import BaseCallback
import torch
import os
import random
import numpy as np
import datagenerator as kdg
import viewportdatagenerator as vdg
import service_train_ver2
import time
import pandas as pd
from stable_baselines3.common.callbacks import BaseCallback
from datetime import datetime
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
SEED = 1
random.seed(SEED) #ëœë¤ ì‹œë“œ ê³ ì •
np.random.seed(SEED)

#filesize,bitrate,qp
file_size, bitrate, q_list=kdg.generateData()

tile_popularity=kdg.generateRoiPopularity()
num_video=kdg.tot_num_video
seg_popularity=kdg.getSegsPopularityList(np.arange(num_video))
num_segs_every_video=kdg.num_segs_every_video #2s ì„¸ê·¸ë¨¼íŠ¸
#ëª¨ë“  ë¹„ë””ì˜¤ì˜ ì´ ì„¸ê·¸ ìˆ˜
tot_num_segs=sum(num_segs_every_video)
num_tile_per_seg=kdg.num_tile_per_seg
num_ver_per_tile=kdg.num_ver_per_tile
num_ver_per_seg=num_tile_per_seg*num_ver_per_tile
#ëª¨ë“  ë²„ì „ íŒŒì¼ ìˆ˜ëŸ‰ ê³„ì‚°
tot_num_vers=sum(num_segs_every_video)*num_tile_per_seg*num_ver_per_tile
vers_popularity=kdg.getTilesPopularity()
num_bw_class=20
bw_mu=11#13
bw_sigma=2#2
bandwidth_class=kdg.getTrainBandwidthClass(num_bw_class,bw_mu,bw_sigma)
#bandwidth_class=kdg.sample_logn_trunc(num_bw_class, mu_log=2.346369415862733, sigma_log=0.3114876058248091, tau=7.0, seed=1)
print(bandwidth_class)
#exit(2)
test_weight_sum=0

print('ppo')
#ëª¨ë“  ë²„ì „ íŒŒì¼ ìˆ˜ëŸ‰ ê³„ì‚°
tot_num_vers=sum(num_segs_every_video)*num_tile_per_seg*num_ver_per_tile
space_limit=kdg.space_limit

def random_select(size):
    return random.randint(0,size-1)



# ========================== ç¯å¢ƒå®šä¹‰ ==========================
class MyEnv(gym.Env):
    def __init__(self):
        #super().__init__()
        super(MyEnv, self).__init__()
        #self.local_random = random.Random()
        #self.local_random.seed(time.time() + os.getpid() + id(self))
        print('training data selection')
        print('trainPopularityset generate start')
        self.trainPopularityset = kdg.generate_video_rank_train_data()
        #self.trainPopularityset=kdg.getRealDatasetRankData()
        print('trainPopularityset generate end')
        self.num_training_seg = 300 #è®­ç»ƒæ ·æœ¬segment æ•°é‡
        self.training_seg_list = np.random.randint(0, tot_num_segs, self.num_training_seg) #åˆå§‹åŒ–æ—¶ç›´æ¥éšæœºé€‰å–num_training_seg
        
        # åŸºæœ¬å‚æ•°
        self.num_ver_per_seg = num_ver_per_seg  # ä¸€ä¸ª segment æœ‰å¤šå°‘ä¸ªç‰ˆæœ¬
        self.num_tile_per_seg = num_tile_per_seg  # ä¸€ä¸ª segment æœ‰å¤šå°‘ tile
        self.num_ver_per_tile = num_ver_per_tile  # æ¯ä¸ª tile æœ‰å¤šå°‘ä¸ªç‰ˆæœ¬
        self.file_size = file_size  # æ‰€æœ‰ç‰ˆæœ¬çš„æ–‡ä»¶å¤§å°
        self.q_list = q_list  # æ‰€æœ‰ç‰ˆæœ¬çš„è´¨é‡åˆ—è¡¨
        self.bitrate = bitrate  # æ‰€æœ‰ç‰ˆæœ¬çš„ç ç‡
        self.tile_popularity = tile_popularity  # æ‰€æœ‰ segment çš„ tile çƒ­åº¦åˆ†å¸ƒ
        self.bandwidth_class = bandwidth_class  # æ¨¡æ‹Ÿçš„å®¢æˆ·ç«¯å¸¦å®½ç­‰çº§åˆ†å¸ƒ
        self.vp_tiles = vp_tiles  # æ‰€æœ‰segmentçš„viewport tileä¿¡æ¯
        self.vp_bitmap = vp_bitmap  # æ‰€æœ‰segment çš„viewport bitmapï¼Œç”¨äºè®¡ç®—é‡è¦åŒºåŸŸ
        self.capacity = 0  # segmentåˆ†é…çš„ç¼“å­˜é™åˆ¶
        self.space_limit_rate_list = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9] #space_limit_rate  # å‚¨å­˜ä¸Šé™æ¯”ä¾‹ï¼ˆå¤‡ç”¨ï¼‰
        self.space_limit_rate_idx=0
        self.seg_no = 0  # å½“å‰ segment çš„ç¼–å·
        self.init_seg_p = 0  # å½“å‰ segment çš„çƒ­åº¦
        self.core_server_request = 0
        self.scale_factor = 100  # å¥–åŠ±ç¼©æ”¾å› å­
        self.select_ver_vector=np.zeros(self.num_ver_per_seg)
        # çŠ¶æ€ç»´åº¦è®¾ç½®ï¼šç‰ˆæœ¬é€‰æ‹©çŠ¶æ€ + æ–‡ä»¶å¤§å° + q_list + å‚¨å­˜é™åˆ¶ + å·²ç”¨ç©ºé—´ + å¸¦å®½å‡å€¼æ–¹å·® + tile çƒ­åº¦ + segment çƒ­åº¦
        self.state_dim = self.num_ver_per_seg * 3 + 2 + 2 + self.num_tile_per_seg + 1
        #self.state = np.zeros(self.state_dim, dtype=np.float32)
        
        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(self.state_dim,), dtype=np.float32)  # çŠ¶æ€ç©ºé—´å½’ä¸€åŒ–
        self.action_space = spaces.Discrete(self.num_ver_per_seg)  # åŠ¨ä½œç©ºé—´ï¼šç¦»æ•£é€‰æ‹©ç‰ˆæœ¬
        # çŠ¶æ€å‘é‡ä¸­å„æ®µç´¢å¼•ä½ç½®
        #0-num_ver_per_seg ç‰ˆæœ¬é€‰æ‹©å‘é‡
        
        self.file_size_idx = self.num_ver_per_seg
        self.qoe_idx = self.num_ver_per_seg * 2
        self.space_limit_idx = self.num_ver_per_seg * 3
        self.space_sum_idx = self.space_limit_idx + 1
        self.bw_mu_idx = self.space_sum_idx + 1
        self.bw_sigma_idx = self.bw_mu_idx + 1
        self.tiles_p_idx = self.bw_sigma_idx + 1
        self.init_seg_p_idx = self.tiles_p_idx + self.num_tile_per_seg
        self.episode_cnt=0
        self.same_seg_episode=0
        # åˆå§‹åŒ–ï¼šæ¯ä¸ªç¼“å­˜ç‡å¯¹åº”ä¸€ä¸ª reward åˆ—è¡¨
        self.reward_record = [[] for _ in range(len(self.space_limit_rate_list))]
        self.space_limit_rate=None
        print('inin complete')
        if(is_train):
            self.reset()
    def setSegNo(self,_segno):
        self.seg_no=_segno
    def setSpaceLimitRate(self,_rate):
        self.space_limit_rate=_rate
    def reset(self, seed=None, options=None):
        # åˆå§‹åŒ–çŠ¶æ€
        #tmp_seg_no = self.training_seg_list[random_select(len(self.training_seg_list))]
        #tmp_space_limit_rate_idx = random_select(len(self.space_limit_rate_list))
        #print(tmp_seg_no)
        if(is_train==True and self.same_seg_episode==0):
            #random.seed(time.time() + os.getpid() + id(self))
            self.same_seg_episode_start_time=time.time()
            self.seg_no = self.training_seg_list[random_select(len(self.training_seg_list))]
            self.space_limit_rate_idx = random_select(len(self.space_limit_rate_list))
            self.space_limit_rate=self.space_limit_rate_list[self.space_limit_rate_idx]
        self.state = np.zeros(self.state_dim, dtype=np.float32)
        self.status = np.zeros(self.num_ver_per_seg, dtype=bool)  # æ¯ä¸ªç‰ˆæœ¬æ˜¯å¦è¢«é€‰æ‹©

        self.select_ver_vector=np.zeros(self.num_ver_per_seg)
        self.used_capacity = 0.0  # å·²ç”¨ç¼“å­˜å®¹é‡
        self.pre_qoe = 0.0  # ä¸Šä¸€æ­¥çš„è§†é¢‘ QoE
        self.seg_start_in_ver = self.seg_no * num_ver_per_seg
        self.seg_end_in_ver = self.seg_start_in_ver + num_ver_per_seg
        # åˆå§‹åŒ–çŠ¶æ€å­—æ®µ
        self.file_size_for_episode=self.file_size[self.seg_start_in_ver:self.seg_end_in_ver]
        self.capacity=self.space_limit_rate*np.sum(self.file_size_for_episode)
        #print('capacity: ',self.capacity)
        self.q_list_for_episode=self.q_list[self.seg_start_in_ver:self.seg_end_in_ver]
        self.bitrate_for_episode=self.bitrate[self.seg_start_in_ver:self.seg_end_in_ver]
        self.file_size_max = np.max(self.file_size_for_episode)
        self.q_list_max = np.max(self.q_list_for_episode)
        self.tile_popularity_for_episode=self.tile_popularity[self.seg_no]
        self.tile_p_max=np.max(self.tile_popularity_for_episode)
        
        # é˜²æ­¢é™¤ä»¥ 0
        self.file_size_for_episode_norm = self.file_size_for_episode / self.file_size_max
        self.q_list_for_episode_norm = self.q_list_for_episode / self.q_list_max
        self.tile_popularity_for_episode_norm=self.tile_popularity_for_episode/self.tile_p_max
        
        
        self.state[self.file_size_idx:self.file_size_idx + self.num_ver_per_seg] = self.file_size_for_episode_norm
        self.state[self.qoe_idx:self.qoe_idx + self.num_ver_per_seg] = self.q_list_for_episode_norm
        self.state[self.space_limit_idx] = self.space_limit_rate
        self.state[self.space_sum_idx] = 0.0  # åˆå§‹ä½¿ç”¨ç©ºé—´ä¸º 0
        self.state[self.bw_mu_idx] = bw_mu/bw_mu # è®¾å®šå›ºå®šçš„å¸¦å®½å‡å€¼å’Œæ–¹å·®ï¼ˆå¯æ›¿æ¢ï¼‰
        self.state[self.bw_sigma_idx] = bw_sigma/bw_mu
        self.init_seg_p=seg_popularity[self.seg_no]
        self.state[self.init_seg_p_idx] = self.init_seg_p
        self.state[self.tiles_p_idx:self.tiles_p_idx + self.num_tile_per_seg] = self.tile_popularity_for_episode_norm
        self.vp_tiles_for_episode=vp_tiles_list[self.seg_no]
        self.vp_bitmap_for_episode=vp_bitmap[self.seg_no]
        if(is_train==True and self.same_seg_episode==0):
            self.core_server_request = service_train_ver2.service_train(np.full(num_ver_per_seg, 1), num_tile_per_seg,
                                                                   num_ver_per_tile,
                                                                   self.bitrate_for_episode,
                                                                   self.q_list_for_episode,
                                                                   self.vp_tiles_for_episode, self.vp_bitmap_for_episode,
                                                                   bandwidth_class)
        
        self.pre_gain=0

        self.p_weight=0
        self.pre_reward=0
        for period in range(24):
            # train_p_samples[period] --> sample no
            self.p_weight += self.trainPopularityset[period][self.seg_no]
        
        
        
        return self.state.copy(), {"action_mask": self.get_valid_action_mask()}

    def step(self, action):
        # å¦‚æœå·²é€‰æ‹©è¯¥ç‰ˆæœ¬æˆ–è¶…å‡ºç¼“å­˜å®¹é‡ï¼Œç»ˆæ­¢æœ¬å›åˆ
        weight = self.file_size_for_episode[action]
        info={}
        if self.used_capacity + weight > self.capacity:
            done=True
            if(done):
                if(is_train==True and self.same_seg_episode%50==0):
                    print('--------------------------------')
                    print('episode',self.episode_cnt,'seg_no',self.seg_no)
                    print('same_seg_episode',self.same_seg_episode)
                    print('self.space_limit_rate',self.space_limit_rate)                
                    print('self.cache gain',self.pre_gain)
                    print('select num_action',np.sum(self.status))
                self.reward_record[self.space_limit_rate_idx].append(self.pre_gain)
                self.episode_cnt+=1
                self.same_seg_episode+=1
                if is_train==False:
                    info["select_vertor"]=self.state[:self.num_ver_per_seg].copy()
                if(is_train==True and self.same_seg_episode==300):
                    self.same_seg_episode_end_time=time.time()
                    print('seg training time',self.same_seg_episode_end_time-self.same_seg_episode_start_time)
                    self.same_seg_episode=0
                #print(self.status)
                #print(self.state[:self.num_ver_per_seg])
            return self.state.copy(), self.pre_reward, done, False, info

        # æ›´æ–°çŠ¶æ€å‘é‡ä¸æ ‡å¿—
        self.status[action] = True
        self.used_capacity += weight
        #print(self.used_capacity)
        self.state[action] = 1.0
        self.state[self.space_sum_idx] = self.used_capacity/self.capacity

        # ä½¿ç”¨ service_train_ver2 ä¸­é»‘ç®±å‡½æ•°è®¡ç®— QoE
        gain=0
        if(is_train==True):
            gain = service_train_ver2.service_train_QBver(
                self.state[:self.num_ver_per_seg],
                self.num_tile_per_seg,
                self.num_ver_per_tile,
                self.bitrate_for_episode,
                self.q_list_for_episode,
                self.vp_tiles_for_episode,
                self.vp_bitmap_for_episode,
                self.bandwidth_class,
                self.core_server_request
            )
   
            # å¥–åŠ±ä¸º QoE å¢ç›Š * segment çƒ­åº¦ * ç¼©æ”¾å› å­
            #reward = (gain - self.pre_gain) * self.p_weight * self.scale_factor/self.p_weight
        reward = (gain - self.pre_gain) * self.scale_factor
        self.pre_gain = gain
        self.pre_reward=reward
        done = np.sum(self.status) >= self.num_ver_per_seg  # å¦‚æœæ‰€æœ‰ç‰ˆæœ¬å¤„ç†å®Œï¼Œåˆ™ç»ˆæ­¢
        
        if(done):
            np.set_printoptions(suppress=True, precision=6)
            if(is_train==True and self.same_seg_episode%50==0):
                print('--------------------------------')
                print('episode',self.episode_cnt,'seg_no',self.seg_no)
                print('same_seg_episode',self.same_seg_episode)
                print('self.space_limit_rate',self.space_limit_rate)
                #print('--------------------------------')
                print('episode_cnt',self.episode_cnt)
                print('self.seg_no',reward)
                print('self.cache gain',gain)
                print('select num_action',np.sum(self.status))
            self.reward_record[self.space_limit_rate_idx].append(gain)
            self.episode_cnt+=1
            self.same_seg_episode+=1
            if is_train==False:
                info["select_vertor"]=self.state[:self.num_ver_per_seg].copy()
            #print(self.status)
            if(is_train==True and self.same_seg_episode==300):
                self.same_seg_episode_end_time=time.time()
                print('seg training time',self.same_seg_episode_end_time-self.same_seg_episode_start_time)
                self.same_seg_episode=0
        return self.state.copy(), reward, done, False, info

    def get_valid_action_mask(self):
        return ~self.status  # è¿”å›æœªé€‰æ‹©çš„ç‰ˆæœ¬ä¸ºåˆæ³•åŠ¨ä½œ
    def get_select_vertor(self):
        return self.state[:self.num_ver_per_seg]


class RewardLoggingCallback(BaseCallback):
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.episode_rewards = []

    def _on_step(self) -> bool:
        # å¦‚æœä¸€ä¸ª episode ç»“æŸï¼Œå°±è®°å½•è¯¥ episode çš„ reward æ€»å’Œ
        if self.locals.get("dones") is not None and any(self.locals["dones"]):
            reward_sum = sum(self.locals["rewards"])
            self.episode_rewards.append(reward_sum)
        return True


# åœ¨ main å‡½æ•°æœ€ä¸Šæ–¹è®¾ç½®
#set_seed(42)
# ========================== ä¸»ç¨‹åºå…¥å£ ==========================
if __name__ == '__main__':
    print('viewport data generate start')
    vp_start_time = time.time()
    vp_tiles_list=[]
    vp_bitmap=[]
    #ì„¸ê·¸ë¨¼íŠ¸ë§ˆë‹¤ ë¯¸ë¦¬ viewportìƒì„±
    for i in range(tot_num_segs):
        vp_tiles= vdg.viewportDataGenerator(num_tile_per_seg, tile_popularity[i],
                                                           len(bandwidth_class))
        bitmap=[]

        for r in range(len(bandwidth_class)):
            bitmap_per_request = []
            for j in range(num_tile_per_seg):
                if j in vp_tiles[r]:
                    bitmap_per_request.append(1)
                else:
                    bitmap_per_request.append(0)
            bitmap.append(bitmap_per_request)
        vp_bitmap.append(bitmap)
        vp_tiles_list.append(vp_tiles)
       
    print('viewport data generate end')
    print('training start')


    # ===================== ğŸ¯ è‡ªå®šä¹‰ callbackï¼šè®°å½•æ¯ä¸ª episode çš„ reward =====================


    # ===================== ğŸ› ï¸ è®­ç»ƒå‚æ•°ä¸è·¯å¾„è®¾å®š =====================
    is_train = False     # æ˜¯å¦è®­ç»ƒ
    #error_ratio = 0.1   # çƒ­åº¦é¢„æµ‹è¯¯å·®æ¯”ä¾‹
    model_path = "ppo_model_knapsack_ver9_0.6.pth"  # æ¨¡å‹ä¿å­˜è·¯å¾„
    env_path = "vecnormalize_knapsack_ver9_0.6.pkl" # å½’ä¸€åŒ–å™¨ä¿å­˜è·¯å¾„
    device = "cpu" if torch.backends.mps.is_available() else "cpu"

    #print("error_ratio", error_ratio)

    # ===================== ğŸŒ± ç¯å¢ƒå°è£…å‡½æ•° =====================


    # åˆ›å»ºåŸå§‹ç¯
    
    def make_env():
        def _init():
            env = MyEnv()
            return ActionMasker(env, lambda env: env.get_valid_action_mask())
        return _init

    # æ„é€ å¹¶åŒ…è£…ç¯å¢ƒ
    num_envs = 1
    #env = DummyVecEnv([make_env(i) for i in range(num_envs)])
    env = DummyVecEnv([make_env() for _ in range(num_envs)])

    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_reward=1.0)
    policy_kwargs = {
    "net_arch": [256, 256]  # ä»…å®šä¹‰éšè—å±‚å¤§å°
    }

    # ===================== ğŸ¤– PPOæ¨¡å‹æ„å»º =====================
    model = MaskablePPO(
        "MlpPolicy",
        env,
        verbose=1,
        device=device,
        policy_kwargs=policy_kwargs,
        n_steps=4096,          # ä¸€æ¬¡ rollout çš„æ­¥æ•°
        batch_size=2048,       # æ‰¹å¤„ç†å¤§å°
        n_epochs=20,           # æ¯æ¬¡æ›´æ–°è½®æ•°
        learning_rate=3e-4,    # å­¦ä¹ ç‡
        gamma=0.99,            # æŠ˜æ‰£å› å­
        gae_lambda=0.95,       # GAE å‚æ•°
        clip_range=0.2,        # PPO æˆªæ–­èŒƒå›´
        ent_coef=0.005,        # entropy æŸå¤±ç³»æ•°
        vf_coef=0.5,           # value function æŸå¤±ç³»æ•°
        max_grad_norm=0.5,     # æœ€å¤§æ¢¯åº¦è£å‰ª
        target_kl=0.02         # KLç›®æ ‡
    )

    # ===================== ğŸš€ è®­ç»ƒæµç¨‹ =====================
    train_time_start=time.time()
    import matplotlib.pyplot as plt
    print("å¼€å§‹æ—¶é—´:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    if is_train:
        callback = RewardLoggingCallback()
        model.learn(total_timesteps=17000000)
        train_time_end=time.time()
        print("ç»“æŸæ—¶é—´:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        print(train_time_end-train_time_start)
        # ä¿å­˜æ¨¡å‹ä¸å½’ä¸€åŒ–çŠ¶æ€
        model.save(model_path)
        env.save(env_path)
        
        print(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ: {model_path}")
        print(f"âœ… ç¯å¢ƒå½’ä¸€åŒ–å™¨ä¿å­˜æˆåŠŸ: {env_path}")
        env_instance = env.venv.envs[0].env.unwrapped

        # è·å– reward è®°å½•ä¸ç¼“å­˜ç‡åˆ—è¡¨
        reward_record = env_instance.reward_record
        space_limit_rate_list = env_instance.space_limit_rate_list

        # ä¸ºæ¯ä¸ªç¼“å­˜ç‡ç”»ä¸€ä¸ªå›¾
        for i, rewards in enumerate(reward_record):
            if len(rewards) == 0:
                continue
            plt.figure()
            plt.plot(range(len(rewards)), rewards, color='orange')
            plt.xlabel("Episode")
            plt.ylabel("Reward")
            plt.title(f"Reward Over Time (Cache Limit: {space_limit_rate_list[i]})")
            plt.grid(True)
            plt.tight_layout()
            plt.show()
        
    else:
        
        # ===================== ğŸ” æµ‹è¯•æµç¨‹ =====================
        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}")
        model = MaskablePPO.load(model_path, device=device)

        print(f"ğŸ“¥ åŠ è½½ç¯å¢ƒå½’ä¸€åŒ–å™¨: {env_path}")
        env = DummyVecEnv([make_env() for _ in range(num_envs)])
        env = VecNormalize.load(env_path, env)
        env.training = False
        env.norm_reward = False
        rate=0.6
        tag = int(rate * 100)                 # 0.3 -> 30, 0.2 -> 20, 0.1 -> 10
        
        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ä¸è¾“å‡ºæ–‡ä»¶
        os.makedirs("sb3stateinfo", exist_ok=True)
        output_path = os.path.join("sb3stateinfo", f"{int(rate*100)}state.txt")

        with open(output_path, "a") as f:
            seg_start_time=time.time()
            for seg_id in range(tot_num_segs):  # å‡è®¾æœ‰ 50 ä¸ª segment
                
                env.envs[0].setSegNo(seg_id)  # è®¾ç½®å½“å‰ segment ç¼–å·
                env.envs[0].setSpaceLimitRate(rate)
                
                obs = env.reset()             # è·å–åˆå§‹çŠ¶æ€
                while True:
                    mask = env.env_method("get_valid_action_mask")[0]
                    a, _ = model.predict(obs, deterministic=True,action_masks=mask)
                    #print(a)
                    obs, r, dones, info = env.step(a)  # VecEnv: donesæ˜¯æ•°ç»„
                    #print(obs)
                    if dones[0]:
                        # ä¼˜å…ˆ state[:num_ver_per_seg]ï¼Œæ²¡æœ‰å°±ç”¨ status
                        #arr = env.envs[0].status
                        #print("info keys:", info[0].keys())
                        arr = info[0]["select_vertor"]
                        #print(arr)
                        f.write(" ".join(str(int(x)) for x in np.array(arr).reshape(-1)) + "\n")
                        break
                if((seg_id+1)%100==0):
                    seg_end_time=time.time()
                    print('seg_id',seg_id,'exec time: ',seg_end_time-seg_start_time)
                    seg_start_time=time.time()
            #exit(1)
        print(f"ğŸ“„ æ‰€æœ‰ç‰ˆæœ¬é€‰æ‹©å·²ä¿å­˜è‡³ {output_path}")